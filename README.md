ğŸ“¦ Fashion-MNIST Classification Project

This project explores how model complexity affects performance in image classification tasks using the Fashion-MNIST dataset. Three deep learning models were built, trained, and compared:

ANN (Artificial Neural Network)

Basic CNN

Deeper CNN

The goal is to understand how convolutional layers improve performance and whether increased depth always leads to better results.

ğŸš€ Project Overview

Fashion-MNIST is a dataset of 70,000 grayscale images (28Ã—28) belonging to 10 different fashion categories.
This project builds multiple neural network architectures and compares them using:

Accuracy

Loss

Training curves

Confusion matrices

Prediction analysis

ğŸ“š Agenda & Steps
1. Dataset Setup

Import all necessary libraries.

Load the Fashion-MNIST dataset.

Normalize pixel values to the range 0â€“1.

Reshape images for CNN input.

One-hot encode labels.

Verify the shapes of processed data.

2. Model Building

Three models with increasing complexity were created:

ğŸ”¹ 1) Basic ANN Model

Flatten layer

Dense(128, ReLU)

Dense(64, ReLU)

Dense(10, Softmax)

Works, but not ideal for image data.

ğŸ”¹ 2) Basic CNN Model

Conv2D â†’ MaxPooling

Conv2D â†’ MaxPooling

Dense layers

Significantly better accuracy than ANN.

ğŸ”¹ 3) Deeper CNN Model

Additional Conv2D layers

Batch Normalization

Dropout

Larger capacity to capture features

Expected to perform best (but results depend on dataset size and regularization).

3. Model Training

Models trained using training + validation split.

EarlyStopping used to avoid overfitting.

ModelCheckpoint used to save best model based on validation loss.

History stored for accuracy/loss visualization.

4. Model Evaluation

After training, the best saved weights for each model were loaded and tested on the Fashion-MNIST test set.

Evaluation included:

Test accuracy & loss

Accuracy and loss curves

Confusion matrix visualizations

Class-wise performance comparison

5. Prediction Analysis

Using the best model (Basic CNN):

Predictions were generated on test images.

Correct and incorrect predictions were visualized.

This helped understand model strengths and weaknesses for each class.

ğŸ† Results & Conclusion
Key Findings

The Basic CNN model performed the best overall, giving the highest accuracy and lowest loss.

The ANN model was simple and fast but performed worse than both CNN models.

The Deeper CNN model, although more complex, did not outperform the Basic CNN consistently.
Reasons could include:

Dataset size

Overfitting

Architectural choices

Excessive regularization

Final Conclusion

A moderately complex CNN (Basic CNN) is ideal for Fashion-MNIST.
More layers do not always guarantee better performance.

CNNs clearly outperform ANNs for image classification tasks.

ğŸ“ Project Structure
â”œâ”€â”€ data_preprocessing.ipynb
â”œâ”€â”€ ann_model.ipynb
â”œâ”€â”€ cnn_basic_model.ipynb
â”œâ”€â”€ cnn_deeper_model.ipynb
â”œâ”€â”€ training_history_plots/
â”œâ”€â”€ confusion_matrices/
â”œâ”€â”€ predictions_visualization/
â””â”€â”€ README.md


(Adjust based on your project folder.)

ğŸ§° Technologies Used

Python

TensorFlow / Keras

NumPy, Pandas

Matplotlib, Seaborn

Scikit-learn

ğŸ“Œ Future Improvements

Hyperparameter tuning

Using data augmentation

Trying ResNet / MobileNet architectures

Applying transfer learning

Using mixed-precision training for faster results
